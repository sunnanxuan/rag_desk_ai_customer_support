from dotenv import load_dotenv
load_dotenv()

import os
import json
import shutil
from pathlib import Path
from datetime import datetime
import streamlit as st
from utils import get_kb_names


if not st.session_state.get("_page_title_set"):
    st.set_page_config(
        page_title="RAG Desk Â· AI Customer Support",
        page_icon="img/small_logo.png",
        layout="wide"
    )
    st.session_state["_page_title_set"] = True

# ========= å›ºå®šçš„åç«¯å‚æ•°ï¼ˆä¸åœ¨ç½‘é¡µæ˜¾ç¤ºï¼‰=========
DEFAULT_CHUNK_SIZE = 1200
DEFAULT_CHUNK_OVERLAP = 200

# ========= çŸ¥è¯†åº“æ ¹ç›®å½•ï¼Œå¯æŒ‰éœ€ä¿®æ”¹ =========
KB_ROOT = Path("knowledge_bases")

# ========= é¢„è®¾çš„ OpenAI Embedding æ¨¡å‹ =========
PRESET_OPENAI_EMBED_MODELS = [
    ("text-embedding-3-small Â· 1536 ç»´ï¼ˆé»˜è®¤ï¼‰", "text-embedding-3-small"),
    ("text-embedding-3-large Â· 3072 ç»´ï¼ˆæ›´é«˜è´¨é‡ï¼‰", "text-embedding-3-large"),
    ("text-embedding-ada-002 Â· 1536 ç»´ï¼ˆæ—§ç‰ˆå…¼å®¹ï¼‰", "text-embedding-ada-002"),
    ("è‡ªå®šä¹‰â€¦", "__custom__"),
]


# =============== åŸºç¡€å·¥å…·å‡½æ•° ===============
def _slugify(name: str) -> str:
    import re
    s = name.strip().lower()
    s = re.sub(r"[^\w\-]+", "-", s)
    s = re.sub(r"-{2,}", "-", s).strip("-")
    return s or "kb"

def _ensure_dirs(p: Path):
    p.mkdir(parents=True, exist_ok=True)

def _save_uploaded_files(files, dest_dir: Path):
    saved_paths = []
    for f in files:
        filename = Path(f.name).name
        out = dest_dir / filename
        with open(out, "wb") as w:
            w.write(f.read())
        saved_paths.append(out)
    return saved_paths

def _load_texts(paths):
    """
    è¯»å–æ–‡ä»¶ä¸ºçº¯æ–‡æœ¬ï¼›æ”¯æŒ .txt, .md, .pdf, .docx
    """
    docs, metas = [], []
    for p in paths:
        p = Path(p)
        ext = p.suffix.lower()
        text = ""

        try:
            if ext in [".txt", ".md"]:
                text = p.read_text(encoding="utf-8", errors="ignore")

            elif ext == ".pdf":
                try:
                    from pypdf import PdfReader  # pip install pypdf
                except Exception as e:
                    raise RuntimeError("ç¼ºå°‘ä¾èµ– pypdfï¼Œè¯·å…ˆå®‰è£…ï¼špip install pypdf") from e
                reader = PdfReader(str(p))
                text = "\n".join(page.extract_text() or "" for page in reader.pages)

            elif ext == ".docx":
                try:
                    import docx  # pip install python-docx
                except Exception as e:
                    raise RuntimeError("ç¼ºå°‘ä¾èµ– python-docxï¼Œè¯·å…ˆå®‰è£…ï¼špip install python-docx") from e
                d = docx.Document(str(p))
                text = "\n".join(par.text for par in d.paragraphs)

            else:
                # å…¶ä»–æ ¼å¼å¯æŒ‰éœ€æ‰©å±•
                continue

        except Exception as e:
            st.warning(f"è¯»å–æ–‡ä»¶å¤±è´¥ï¼š{p.name}ï¼ŒåŸå› ï¼š{e}")
            continue

        if text and text.strip():
            docs.append(text)
            metas.append({"source": str(p), "filename": p.name, "ext": ext})
    return docs, metas

def _split_texts(texts, metas, chunk_size=DEFAULT_CHUNK_SIZE, chunk_overlap=DEFAULT_CHUNK_OVERLAP):
    try:
        from langchain_text_splitters import RecursiveCharacterTextSplitter
    except Exception as e:
        raise RuntimeError(
            "ç¼ºå°‘ langchain-text-splittersï¼Œè¯·å®‰è£…ï¼špip install langchain-text-splitters"
        ) from e

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=int(chunk_size),
        chunk_overlap=int(chunk_overlap),
        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼Œ", " ", ""],
    )
    chunk_texts, chunk_metas = [], []
    for t, m in zip(texts, metas):
        chunks = splitter.split_text(t)
        chunk_texts.extend(chunks)
        chunk_metas.extend([m] * len(chunks))
    return chunk_texts, chunk_metas

def _get_embeddings_openai(model_name: str):
    try:
        from langchain_openai import OpenAIEmbeddings
    except Exception as e:
        raise RuntimeError("ç¼ºå°‘ langchain-openaiï¼Œè¯·å®‰è£…ï¼špip install langchain-openai") from e
    if not os.getenv("OPENAI_API_KEY"):
        st.warning("æœªæ£€æµ‹åˆ° OPENAI_API_KEY ç¯å¢ƒå˜é‡ï¼Œåç»­å¯èƒ½æŠ¥ 401ã€‚è¯·åœ¨ .env æˆ–ç³»ç»Ÿç¯å¢ƒä¸­è®¾ç½®ã€‚")
    return OpenAIEmbeddings(model=model_name)

def _build_chroma(texts, metas, persist_dir: Path, embeddings):
    try:
        from langchain_community.vectorstores import Chroma
    except Exception as e:
        raise RuntimeError(
            "ç¼ºå°‘ chroma ä¾èµ–ï¼Œè¯·å®‰è£…ï¼špip install chromadb langchain-community"
        ) from e

    _ensure_dirs(persist_dir)
    vs = Chroma.from_texts(
        texts=texts,
        embedding=embeddings,
        metadatas=metas,
        persist_directory=str(persist_dir),
    )
    return vs

def _list_existing_kbs():
    # ä¼˜å…ˆç”¨ utils.get_kb_namesï¼ˆä¸ç°æœ‰é¡¹ç›®ä¿æŒä¸€è‡´ï¼‰
    try:
        names = get_kb_names()
        if isinstance(names, (list, tuple)) and names:
            return sorted(set(_slugify(n) for n in names))
    except Exception:
        pass
    if KB_ROOT.exists():
        return sorted([p.name for p in KB_ROOT.iterdir() if p.is_dir()])
    return []


def _load_meta(kb_dir: Path) -> dict:
    meta_path = kb_dir / "meta.json"
    if meta_path.exists():
        try:
            return json.loads(meta_path.read_text(encoding="utf-8"))
        except Exception:
            return {}
    return {}

def _save_meta(kb_dir: Path, meta: dict):
    meta_path = kb_dir / "meta.json"
    meta_path.write_text(json.dumps(meta, ensure_ascii=False, indent=2), encoding="utf-8")


def knowledge_base_page():
    st.title("è¡Œä¸šçŸ¥è¯†åº“")
    st.caption("ä¸Šä¼ æ–‡æ¡£ï¼Œæ„å»ºå‘é‡åº“ï¼Œç”¨äºå¯¹è¯æ£€ç´¢ï¼ˆRAGï¼‰")

    _ensure_dirs(KB_ROOT)
    st.subheader("â‘  åˆ›å»ºæ–°çš„çŸ¥è¯†åº“")
    with st.form("create_kb_form", clear_on_submit=False):
        kb_name_input = st.text_input("çŸ¥è¯†åº“åç§°ï¼ˆå°†è½¬ä¸ºå°å†™ slugï¼‰", placeholder="ä¾‹å¦‚ï¼šbanking_faq")
        kb_desc_input = st.text_area("çŸ¥è¯†åº“æè¿°ï¼ˆå¯é€‰ï¼‰", placeholder="ä¾‹ï¼šé“¶è¡Œå®¢æœå¸¸è§é—®é¢˜ä¸å›ç­”æ±‡æ€»â€¦", height=80)
        uploaded_files = st.file_uploader(
            "ä¸Šä¼ æ–‡æ¡£ï¼ˆå¯å¤šé€‰ï¼‰",
            type=["txt", "md", "pdf", "docx"],
            accept_multiple_files=True,
            help="æ”¯æŒ .txt, .md, .pdf, .docx",
        )

        label_list = [x[0] for x in PRESET_OPENAI_EMBED_MODELS]
        choice = st.selectbox("é€‰æ‹© OpenAI Embeddings æ¨¡å‹", label_list, index=0)
        mapped = dict(PRESET_OPENAI_EMBED_MODELS)[choice]
        if mapped == "__custom__":
            model_name = st.text_input(
                "è‡ªå®šä¹‰æ¨¡å‹åç§°",
                value="text-embedding-3-small",
                help="å¡«å†™ä»»æ„å¯ç”¨çš„ OpenAI Embeddings æ¨¡å‹å",
            )
        else:
            model_name = mapped

        with st.expander("æ¨¡å‹è¯´æ˜", expanded=False):
            st.markdown(
                "- **text-embedding-3-small**ï¼š1536 ç»´ï¼Œä»·æ ¼æ›´ä½ï¼Œé€‚åˆå¤§å¤šæ•°æ£€ç´¢åœºæ™¯ã€‚\n"
                "- **text-embedding-3-large**ï¼š3072 ç»´ï¼Œæ›´é«˜è´¨é‡ï¼Œé€‚åˆé«˜ç²¾åº¦æ£€ç´¢/è·¨åŸŸè¯­æ–™ã€‚\n"
                "- **text-embedding-ada-002**ï¼šæ—§ç‰ˆï¼Œå‡ºäºå…¼å®¹ä¿ç•™ï¼Œä¸å»ºè®®æ–°é¡¹ç›®ä½¿ç”¨ã€‚\n"
                "å¦‚éœ€å…¶ä»–æ¨¡å‹ï¼Œè¯·é€‰æ‹©â€œè‡ªå®šä¹‰â€¦â€ã€‚"
            )

        submitted = st.form_submit_button("æ„å»ºçŸ¥è¯†åº“", use_container_width=True)

    if submitted:
        kb_name = _slugify(kb_name_input)
        if not kb_name_input.strip():
            st.error("è¯·å¡«å†™çŸ¥è¯†åº“åç§°ã€‚")
            st.stop()
        if not uploaded_files:
            st.error("è¯·è‡³å°‘ä¸Šä¼ ä¸€ä¸ªæ–‡æ¡£ã€‚")
            st.stop()
        if not model_name.strip():
            st.error("è¯·é€‰æ‹©æˆ–å¡«å†™æœ‰æ•ˆçš„ Embeddings æ¨¡å‹åã€‚")
            st.stop()

        kb_dir = KB_ROOT / kb_name
        src_dir = kb_dir / "source"
        chroma_dir = kb_dir / "chroma"

        if kb_dir.exists():
            st.warning(f"çŸ¥è¯†åº“â€œ{kb_name}â€å·²å­˜åœ¨ï¼Œå°†è¦†ç›–å…¶å‘é‡åº“ï¼ˆåŸå§‹æ–‡ä»¶ä¿ç•™ï¼‰ã€‚")

        _ensure_dirs(src_dir)

        with st.status("æ­£åœ¨ä¿å­˜ä¸Šä¼ æ–‡ä»¶â€¦", expanded=False) as s:
            saved_paths = _save_uploaded_files(uploaded_files, src_dir)
            s.update(label=f"å·²ä¿å­˜ {len(saved_paths)} ä¸ªæ–‡ä»¶", expanded=False)

        with st.status("æ­£åœ¨è¯»å–ä¸åˆ‡åˆ†æ–‡æ¡£â€¦", expanded=False) as s:
            texts, metas = _load_texts(saved_paths)
            if not texts:
                s.update(label="æœªèƒ½ä»æ–‡ä»¶ä¸­è¯»å–åˆ°æ–‡æœ¬å†…å®¹ã€‚", state="error")
                st.stop()
            # ä½¿ç”¨å›ºå®šçš„åˆ‡åˆ†å‚æ•°
            chunk_texts, chunk_metas = _split_texts(
                texts, metas,
                chunk_size=DEFAULT_CHUNK_SIZE,
                chunk_overlap=DEFAULT_CHUNK_OVERLAP,
            )
            s.update(label=f"å·²åˆ‡åˆ†ä¸º {len(chunk_texts)} ä¸ªç‰‡æ®µ", expanded=False)

        with st.status("æ­£åœ¨åˆ›å»ºå‘é‡åº“ï¼ˆChromaï¼‰â€¦", expanded=False) as s:
            try:
                embeddings = _get_embeddings_openai(model_name)
            except Exception as e:
                s.update(label=f"åˆ›å»º Embeddings å¤±è´¥ï¼š{e}", state="error")
                st.stop()
            try:
                _build_chroma(chunk_texts, chunk_metas, chroma_dir, embeddings)
            except Exception as e:
                s.update(label=f"æ„å»º Chroma å¤±è´¥ï¼š{e}", state="error")
                st.stop()
            s.update(label="å‘é‡åº“æ„å»ºå®Œæˆ", expanded=False)

        meta = {
            "kb_name": kb_name,
            "description": kb_desc_input.strip(),
            "created_at": datetime.now().isoformat(timespec="seconds"),
            "files": [Path(p).name for p in saved_paths],
            "num_files": len(saved_paths),
            "num_chunks": len(chunk_texts),
            # å›ºå®šçš„åˆ‡åˆ†å‚æ•°å†™å…¥ metaï¼ˆä»…ä¿¡æ¯å±•ç¤ºï¼Œä¸åœ¨ UI æš´éœ²ï¼‰
            "chunk_size": int(DEFAULT_CHUNK_SIZE),
            "chunk_overlap": int(DEFAULT_CHUNK_OVERLAP),
            "embedding_backend": "OpenAI",
            "embedding_model": model_name,
            "persist_directory": str(chroma_dir),
        }
        _save_meta(kb_dir, meta)

        st.success(f"âœ… çŸ¥è¯†åº“â€œ{kb_name}â€å·²æ„å»ºå®Œæˆï¼")
        with st.expander("æ„å»ºæ‘˜è¦", expanded=False):
            st.json(meta, expanded=False)
        st.toast("çŸ¥è¯†åº“åˆ›å»ºæˆåŠŸ", icon="âœ…")

    st.divider()

    # --- â‘¡ ç®¡ç†å·²æœ‰çŸ¥è¯†åº“ ---
    st.subheader("â‘¡ ç®¡ç†å·²æœ‰çŸ¥è¯†åº“")
    existing = _list_existing_kbs()
    if not existing:
        st.info("å½“å‰æ²¡æœ‰å·²å­˜åœ¨çš„çŸ¥è¯†åº“ã€‚")
        return

    col_a, col_b = st.columns([2, 1], vertical_alignment="center")
    with col_a:
        kb_selected = st.selectbox("é€‰æ‹©ä¸€ä¸ªçŸ¥è¯†åº“", existing)
    with col_b:
        show_meta = st.toggle("æ˜¾ç¤ºå…ƒä¿¡æ¯", value=True)

    kb_dir = KB_ROOT / kb_selected
    meta = _load_meta(kb_dir)

    # å…ƒä¿¡æ¯å±•ç¤ºï¼ˆåŒ…å«å›ºå®šçš„ chunk é…ç½®ï¼Œä»…ä¿¡æ¯ç”¨é€”ï¼‰
    if show_meta:
        st.caption("çŸ¥è¯†åº“ä¿¡æ¯")
        if meta:
            st.code(json.dumps(meta, ensure_ascii=False, indent=2), wrap_lines=True)
        else:
            st.write("æœªæ‰¾åˆ° meta.jsonã€‚")

    # åœ¨çº¿ç¼–è¾‘â€œæè¿°â€
    st.markdown("**ç¼–è¾‘æè¿°**")
    new_desc = st.text_area("çŸ¥è¯†åº“æè¿°", value=meta.get("description", ""), height=80, key=f"desc_{kb_selected}")
    if st.button("ä¿å­˜æè¿°", use_container_width=False):
        meta["description"] = new_desc.strip()
        meta["updated_at"] = datetime.now().isoformat(timespec="seconds")
        _save_meta(kb_dir, meta)
        st.success("å·²ä¿å­˜çŸ¥è¯†åº“æè¿°ã€‚")

    # æ“ä½œæŒ‰é’®ï¼šé‡å»º / åˆ é™¤
    c1, c2, _ = st.columns([1, 1, 6])
    with c1:
        if st.button("é‡å»ºå‘é‡åº“", use_container_width=True):
            src_dir = kb_dir / "source"
            chroma_dir = kb_dir / "chroma"
            if not src_dir.exists() or not any(src_dir.iterdir()):
                st.error("æ²¡æœ‰æ‰¾åˆ°åŸå§‹æ–‡ä»¶ï¼ˆsource/ï¼‰ã€‚æ— æ³•é‡å»ºã€‚")
            else:
                with st.status("æ­£åœ¨é‡å»ºå‘é‡åº“â€¦", expanded=False) as s:
                    texts, metas = _load_texts([p for p in src_dir.iterdir() if p.is_file()])
                    if not texts:
                        s.update(label="åŸå§‹æ–‡ä»¶ä¸­æ²¡æœ‰å¯è¯»å–çš„æ–‡æœ¬å†…å®¹ã€‚", state="error")
                        st.stop()

                    # ä»ç„¶ä½¿ç”¨å›ºå®šçš„åˆ‡åˆ†å‚æ•°
                    chunk_texts, chunk_metas = _split_texts(
                        texts, metas,
                        chunk_size=DEFAULT_CHUNK_SIZE,
                        chunk_overlap=DEFAULT_CHUNK_OVERLAP,
                    )

                    model_name_saved = meta.get("embedding_model", "text-embedding-3-small")

                    try:
                        embeddings = _get_embeddings_openai(model_name_saved)
                        if chroma_dir.exists():
                            shutil.rmtree(chroma_dir)
                        _build_chroma(chunk_texts, chunk_metas, chroma_dir, embeddings)
                    except Exception as e:
                        s.update(label=f"é‡å»ºå¤±è´¥ï¼š{e}", state="error")
                        st.stop()

                    s.update(label="é‡å»ºå®Œæˆ", expanded=False)

                meta.update({
                    "num_chunks": len(chunk_texts),
                    "updated_at": datetime.now().isoformat(timespec="seconds"),
                    "persist_directory": str(chroma_dir),
                    # ç¡®ä¿ meta ä¸­åæ˜ å›ºå®šå‚æ•°
                    "chunk_size": int(DEFAULT_CHUNK_SIZE),
                    "chunk_overlap": int(DEFAULT_CHUNK_OVERLAP),
                    "embedding_backend": "OpenAI",
                })
                _save_meta(kb_dir, meta)
                st.success("âœ… å·²é‡å»ºå‘é‡åº“ã€‚")

    with c2:
        if st.button("åˆ é™¤çŸ¥è¯†åº“", type="secondary", use_container_width=True):
            try:
                shutil.rmtree(kb_dir)
                st.success(f"å·²åˆ é™¤çŸ¥è¯†åº“ï¼š{kb_selected}")
                st.toast("åˆ é™¤æˆåŠŸ", icon="ğŸ—‘ï¸")
                st.experimental_rerun()
            except Exception as e:
                st.error(f"åˆ é™¤å¤±è´¥ï¼š{e}")

# ä»…å½“ä½œä¸ºé¡µé¢è¿è¡Œæ—¶æ‰æ¸²æŸ“ï¼›è¢« importï¼ˆå¦‚ app.py å– __file__ï¼‰æ—¶ä¸æ‰§è¡Œ
if __name__ == "__main__":
    knowledge_base_page()
