from dotenv import load_dotenv
load_dotenv()


import json
import streamlit as st
from langgraph.prebuilt import ToolNode, tools_condition
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import StateGraph, MessagesState
from langchain_core.messages import AIMessageChunk, ToolMessage, SystemMessage
from utils import *
from tools.naive_rag_tool import get_naive_rag_tool
from prompts import *
from config import *




if not st.session_state.get("_page_title_set"):
    st.set_page_config(
        page_title="RAG Desk Â· AI Customer Support",
        page_icon="img/small_logo.png",
        layout="wide"
    )
    st.session_state["_page_title_set"] = True





def _list_all_kbs():
    names = set()

    # æ¥æº 1
    try:
        for n in get_kb_names() or []:
            if n and isinstance(n, str):
                names.add(n.strip())
    except Exception:
        pass

    # æ¥æº 2
    try:
        kb_root = os.path.join(os.path.dirname(os.path.dirname(__file__)), "knowledge_bases")
        if os.path.isdir(kb_root):
            for entry in sorted(os.listdir(kb_root)):
                p = os.path.join(kb_root, entry)
                if os.path.isdir(p):
                    names.add(entry.strip())
    except Exception:
        pass

    return sorted(n for n in names if n)

# â€”â€” é¡¶éƒ¨æ¨¡å‹é…ç½®ï¼šé»˜è®¤å€¼ä¸åŠ¨æ€æ ‡ç­¾ â€”â€”
def _init_cfg_defaults():
    if "cfg_platform" not in st.session_state:
        st.session_state["cfg_platform"] = PLATFORMS[0]
    models0 = get_llm_models(st.session_state["cfg_platform"])
    if "cfg_model" not in st.session_state:
        st.session_state["cfg_model"] = (models0[0] if models0 else "")
    if "cfg_temp" not in st.session_state:
        st.session_state["cfg_temp"] = 0.3
    if "cfg_hist_len" not in st.session_state:
        st.session_state["cfg_hist_len"] = 5

def _cfg_label() -> str:
    p = st.session_state.get("cfg_platform", "")
    m = st.session_state.get("cfg_model", "")
    t = st.session_state.get("cfg_temp", 0.3)
    h = st.session_state.get("cfg_hist_len", 5)
    return f"{m}"


def get_rag_graph(platform, model, temperature, selected_kbs, KBS):
    tools = [KBS[k] for k in selected_kbs] if selected_kbs else []
    tool_node = ToolNode(tools) if tools else None

    def call_model(state):
        llm = get_chatllm(platform, model, temperature=temperature)
        llm_with_tools = llm.bind_tools(tools) if tools else llm
        msgs = [SystemMessage(content=RAG_ROUTING_SYSTEM_PROMPT), *state["messages"]]
        return {"messages": [llm_with_tools.invoke(msgs)]}

    workflow = StateGraph(MessagesState)
    workflow.add_node("agent", call_model)
    if tool_node:
        workflow.add_node("tools", tool_node)
        workflow.add_conditional_edges("agent", tools_condition)
        workflow.add_edge("tools", "agent")

    workflow.set_entry_point("agent")
    checkpointer = MemorySaver()
    app = workflow.compile(checkpointer=checkpointer)
    return app

def graph_response(graph, input_messages):
    """æŠŠ LangGraph çš„æµå¼æ¶ˆæ¯è½¬æˆ Streamlit å¯å†™å…¥çš„ç”Ÿæˆå™¨"""
    for event in graph.invoke(
        {"messages": input_messages},
        config={"configurable": {"thread_id": 42}},
        stream_mode="messages",
    ):
        msg = event[0]
        # Agent æ–‡æœ¬å—
        if isinstance(msg, AIMessageChunk):
            if getattr(msg, "tool_calls", None):
                st.session_state["rag_tool_calls"].append(
                    {
                        "status": "æ­£åœ¨æŸ¥è¯¢â€¦â€¦",
                        "knowledge_base": msg.tool_calls[0]["name"].replace(
                            "_knoeledge_base_tool", ""
                        ),
                        "query": "",
                    }
                )
            yield msg.content

        # å·¥å…·è¿”å›ï¼ˆçŸ¥è¯†åº“æ£€ç´¢ç»“æœï¼‰
        elif isinstance(msg, ToolMessage):
            status_placeholder = st.empty()
            with status_placeholder.status("æ­£åœ¨æŸ¥è¯¢â€¦â€¦", expanded=True) as s:
                kb_name = getattr(msg, "name", "").replace(
                    "_knoeledge_base_tool", ""
                ) or "çŸ¥è¯†åº“"
                st.write("å·²è°ƒç”¨", kb_name, "è¿›è¡ŒæŸ¥è¯¢")
                content = json.loads(getattr(msg, "content", ""))
                st.write("çŸ¥è¯†åº“æ£€ç´¢ç»“æœï¼š")
                st.code(
                    content if isinstance(content, str) else json.dumps(content, ensure_ascii=False, indent=2),
                    wrap_lines=True,
                )
                s.update(label="å·²å®ŒæˆçŸ¥è¯†åº“æ£€ç´¢", expanded=False)

            # åˆå¹¶åˆ° session_state çš„å·¥å…·è°ƒç”¨è®°å½•
            if len(st.session_state["rag_tool_calls"]) and "content" not in st.session_state["rag_tool_calls"][-1]:
                st.session_state["rag_tool_calls"][-1]["status"] = "å·²å®ŒæˆçŸ¥è¯†åº“æ£€ç´¢ï¼"
                st.session_state["rag_tool_calls"][-1]["content"] = content
            else:
                st.session_state["rag_tool_calls"].append(
                    {
                        "status": "å·²å®ŒæˆçŸ¥è¯†åº“æ£€ç´¢ï¼",
                        "knowledge_base": kb_name,
                        "content": content,
                    }
                )

def get_rag_chat_response(platform, model, temperature, input_messages, selected_kbs, KBS):
    app = get_rag_graph(platform, model, temperature, selected_kbs, KBS)
    return graph_response(graph=app, input_messages=input_messages)

# -----------------------------
# UIï¼šå†å²è®°å½•/æ¸…ç©º
# -----------------------------
def display_chat_history():
    for message in st.session_state["rag_chat_history_with_too_call"]:
        with st.chat_message(
            message["role"],
            avatar=get_img_base64("img/small_logo.png") if message["role"] == "assistant" else None,
        ):
            # å±•ç¤ºå·¥å…·æ£€ç´¢çŠ¶æ€ï¼ˆå¦‚æœæœ‰ï¼‰
            if "tool_calls" in message:
                for tool_call in message["tool_calls"]:
                    with st.status(tool_call.get("status", "æŸ¥è¯¢ä¸­"), expanded=False):
                        st.write("å·²è°ƒç”¨", tool_call.get("knowledge_base", "çŸ¥è¯†åº“"), "è¿›è¡ŒæŸ¥è¯¢")
                        if "content" in tool_call:
                            st.write("çŸ¥è¯†åº“æŸ¥è¯¢ç»“æœï¼š")
                            st.code(
                                tool_call["content"]
                                if isinstance(tool_call["content"], str)
                                else json.dumps(tool_call["content"], ensure_ascii=False, indent=2),
                                wrap_lines=True,
                            )
            st.write(message.get("content", ""))

def clear_chat_history():
    st.session_state["rag_chat_history"] = [
        {"role": "assistant", "content": RAG_PAGE_INTRODCCTION}
    ]
    st.session_state["rag_chat_history_with_too_call"] = [
        {"role": "assistant", "content": RAG_PAGE_INTRODCCTION}
    ]
    st.session_state["rag_tool_calls"] = []

# -----------------------------
# é¡µé¢ä¸»ä½“
# -----------------------------
def rag_chat_page():
    # --- æ„å»ºå¯ç”¨çš„çŸ¥è¯†åº“å·¥å…· ---
    kbs = _list_all_kbs()
    KBS = {k: get_naive_rag_tool(k) for k in kbs}

    # --- åˆå§‹åŒ–ä¼šè¯çŠ¶æ€ ---
    if "rag_chat_history" not in st.session_state:
        st.session_state["rag_chat_history"] = [
            {"role": "assistant", "content": RAG_PAGE_INTRODCCTION}
        ]
    if "rag_chat_history_with_too_call" not in st.session_state:
        st.session_state["rag_chat_history_with_too_call"] = [
            {"role": "assistant", "content": RAG_PAGE_INTRODCCTION}
        ]
    if "rag_tool_calls" not in st.session_state:
        st.session_state["rag_tool_calls"] = []

    # ================= é¡¶éƒ¨å·¥å…·æ¡ï¼ˆåƒ ChatGPT é¡¶éƒ¨ä¸€æ ·ï¼‰ =================
    _init_cfg_defaults()  # ç¡®ä¿æœ‰é»˜è®¤å€¼
    top_bar = st.container()
    with top_bar:
        left, mid, right = st.columns([6, 4, 2], vertical_alignment="center")
        with left:
            st.markdown("### RAG Desk Â· AI Customer Support")
        with mid:
            # åŠ¨æ€æ˜¾ç¤ºå½“å‰é…ç½®çš„å¼¹å‡ºæŒ‰é’®
            with st.popover(_cfg_label(), use_container_width=True, help="é…ç½®å½“å‰å¯¹è¯ä½¿ç”¨çš„æ¨¡å‹"):
                # å¹³å°é€‰æ‹©
                st.selectbox("åŠ è½½æ–¹å¼ï¼ˆå¹³å°ï¼‰", PLATFORMS, key="cfg_platform")

                # æ ¹æ®å¹³å°åˆ·æ–°æ¨¡å‹æ¸…å•
                _models = get_llm_models(st.session_state["cfg_platform"])
                if _models and st.session_state.get("cfg_model") not in _models:
                    st.session_state["cfg_model"] = _models[0]
                st.selectbox("é€‰æ‹©æ¨¡å‹", _models, key="cfg_model")

                # å…¶å®ƒå‚æ•°
                st.slider("Temperature", 0.1, 1.0, key="cfg_temp")
                st.slider("å†å²æ¶ˆæ¯é•¿åº¦", 1, 10, key="cfg_hist_len")

        with right:
            st.button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯", use_container_width=True, on_click=clear_chat_history)

    # è¯»å–å½“å‰é…ç½®
    platform = st.session_state["cfg_platform"]
    model = st.session_state["cfg_model"]
    temperature = float(st.session_state["cfg_temp"])
    history_len = int(st.session_state["cfg_hist_len"])

    # --- ä¾§è¾¹æ ï¼šçŸ¥è¯†åº“é€‰æ‹© ---
    with st.sidebar:
        if kbs:
            selected_kbs = st.multiselect("è¯·é€‰æ‹©å¯¹è¯ä¸­å¯ä½¿ç”¨çš„çŸ¥è¯†åº“", kbs, default=kbs)
        else:
            st.info("æœªå‘ç°å¯ç”¨çŸ¥è¯†åº“ã€‚è¯·å…ˆåˆ°ã€Œè®¾ç½® â†’ è¡Œä¸šçŸ¥è¯†åº“ã€ä¸Šä¼ å¹¶æ„å»ºã€‚")
            selected_kbs = []

        # å¯é€‰ï¼šå±•ç¤ºæœ€è¿‘ä¸€æ¬¡å·¥å…·è°ƒç”¨çŠ¶æ€
        if st.session_state["rag_tool_calls"]:
            st.divider()
            st.caption("æœ€è¿‘ä¸€æ¬¡çŸ¥è¯†åº“è°ƒç”¨")
            last = st.session_state["rag_tool_calls"][-1]
            st.write("çŠ¶æ€ï¼š", last.get("status", ""))
            st.write("çŸ¥è¯†åº“ï¼š", last.get("knowledge_base", ""))
            if "content" in last:
                with st.expander("æŸ¥çœ‹è¿”å›å†…å®¹"):
                    st.code(
                        last["content"]
                        if isinstance(last["content"], str)
                        else json.dumps(last["content"], ensure_ascii=False, indent=2),
                        wrap_lines=True,
                    )

    # --- å†å²åŒº ---
    display_chat_history()

    # --- è¾“å…¥åŒºï¼ˆé¡µé¢åº•éƒ¨çš„åŸç”Ÿ chat_inputï¼‰ ---
    user_text = st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜â€¦")

    # --- å¤„ç†è¾“å…¥ ---
    if user_text:
        with st.chat_message("user"):
            st.write(user_text)

        st.session_state["rag_chat_history"].append({"role": "user", "content": user_text})
        st.session_state["rag_chat_history_with_too_call"].append({"role": "user", "content": user_text})

        # æˆªå–å†å²é•¿åº¦
        hist = st.session_state["rag_chat_history"][-history_len:]

        stream_response = get_rag_chat_response(
            platform=platform,
            model=model,
            temperature=temperature,
            input_messages=hist,
            selected_kbs=selected_kbs,
            KBS=KBS,
        )

        with st.chat_message("assistant", avatar=get_img_base64("img/small_logo.png")):
            assistant_text = st.write_stream(stream_response)

        # æŠŠæœ€ç»ˆå›ç­”å†™å›ä¼šè¯ï¼ˆç”¨äºåˆ·æ–°å›æ˜¾ï¼‰
        st.session_state["rag_chat_history"].append({"role": "assistant", "content": assistant_text})
        st.session_state["rag_chat_history_with_too_call"].append({"role": "assistant", "content": assistant_text})

# ä»…å½“ä½œä¸ºâ€œé¡µé¢è„šæœ¬â€è¿è¡Œæ—¶æ‰æ¸²æŸ“
if __name__ == "__main__":
    rag_chat_page()
